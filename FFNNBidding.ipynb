{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "FFNNBidding.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unconst/GradientBidding/blob/master/FFNNBidding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsLJvoFe4duQ",
        "colab_type": "text"
      },
      "source": [
        "# FFNN Bidding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2foGUlFqjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "d57d0526-0b44-457e-d199-62b651cf8872"
      },
      "source": [
        "!pip -q install loguru\n",
        "!wget https://github.com/unconst/GradientBidding/raw/master/utils.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.9MB/s \n",
            "\u001b[?25h  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2020-01-23 01:50:22--  https://github.com/unconst/GradientBidding/raw/master/utils.py\n",
            "Resolving github.com (github.com)... 140.82.118.3\n",
            "Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/unconst/GradientBidding/master/utils.py [following]\n",
            "--2020-01-23 01:50:22--  https://raw.githubusercontent.com/unconst/GradientBidding/master/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5305 (5.2K) [text/plain]\n",
            "Saving to: ‘utils.py.1’\n",
            "\n",
            "utils.py.1          100%[===================>]   5.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-01-23 01:50:22 (80.7 MB/s) - ‘utils.py.1’ saved [5305/5305]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EqeHeBv38Ne",
        "colab_type": "code",
        "outputId": "d8f1ffb7-7d19-4014-ef3a-8fa321ca9ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "import numpy\n",
        "import tensorflow as tf\n",
        "import types\n",
        "import time\n",
        "from utils import TBLogger\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.python.framework import ops\n",
        "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)\n",
        "numpy.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-9c2f9bdac64b>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esh8sBEf38Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ffnn(x, hparams):\n",
        "    sizes = [hparams.n_inputs] + [hparams.n_hidden for _ in range(hparams.n_layers)] + [hparams.n_targets]\n",
        "    for i in range(len(sizes) - 1):\n",
        "        w = tf.Variable(tf.truncated_normal(sizes[i:i+2], stddev=0.1))\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[sizes[i+1]]))\n",
        "        x = tf.matmul(x, w) + b\n",
        "        \n",
        "        shift = tf.reduce_mean(w, axis=1, keepdims=True)\n",
        "        relative = w = tf.tile(shifts, [hparams.n_clients, 1])\n",
        "        \n",
        "        shifts = tf.expand_dims(tf.reduce_mean(weights, axis=0), 0)\n",
        "    relative = weights - tf.tile(shifts, [hparams.n_clients, 1]) + hparams.market_shift * tf.ones([hparams.n_clients, 1 + hparams.n_experts]) \n",
        "    masks = tf.nn.relu(relative)\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb2J620S38No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(hparams):    \n",
        "    inputs = tf.placeholder(\"float\", [None, hparams.n_inputs], 'inputs')\n",
        "    targets = tf.placeholder(\"float\", [None, hparams.n_targets], 'targets')   \n",
        "    \n",
        "    masks = []\n",
        "    x = inputs\n",
        "    sizes = [hparams.n_inputs] + [hparams.n_hidden for _ in range(hparams.n_layers)] + [hparams.n_targets]\n",
        "    for i in range(len(sizes) - 1):\n",
        "        \n",
        "        # Layer weights.\n",
        "        w = tf.Variable(tf.truncated_normal(sizes[i:i+2], stddev=0.1))\n",
        "        b = tf.Variable(tf.constant(0.1, shape=[sizes[i+1]]))\n",
        "        \n",
        "        # Layer flows\n",
        "        f = tf.Variable(tf.truncated_normal(sizes[i:i+2], stddev=0.1))\n",
        "        \n",
        "        # Flow from neuron i to j sums to 1. \n",
        "        f = tf.linalg.normalize(f, axis=0, ord=1)[0]\n",
        "        \n",
        "        # Market shift\n",
        "        s = tf.tile(tf.reduce_mean(f, axis=0, keepdims=True), [sizes[i], 1])\n",
        "        relative = f - s\n",
        "        mask = tf.nn.relu(relative)\n",
        "        masks.append(mask)\n",
        "        \n",
        "        # Apply the mask to the weights.\n",
        "        w = tf.multiply(w, mask)\n",
        "        \n",
        "        # Use the weights.\n",
        "        x = tf.matmul(x, w) + b \n",
        "                \n",
        "    logits = x\n",
        "        \n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=targets, logits=logits))\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "    full_loss = loss + (hparams.alpha * norm_sum)\n",
        "    \n",
        "    train_step = tf.train.AdamOptimizer(hparams.learning_rate).minimize(full_loss)\n",
        "    \n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "    }\n",
        "    for i,m in enumerate(masks):\n",
        "        metrics['sparsity_in_mask_' + str(i)] = tf.nn.zero_fraction(m)\n",
        "        \n",
        "    return train_step, metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dXVER1t38Nr",
        "colab_type": "code",
        "outputId": "dc52e6fd-b312-4bad-e9d7-940769dd5352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "hparams = types.SimpleNamespace( \n",
        "    batch_size=256,\n",
        "    learning_rate=1e-3,\n",
        "    n_inputs = 784,\n",
        "    n_targets = 10,\n",
        "    n_layers = 2,\n",
        "    n_hidden = 256,\n",
        "    n_iterations = 1000000,\n",
        "    n_print = 1000,\n",
        "    alpha = 0.00001,\n",
        "    logdir='logs/' + str(int(time.time()))\n",
        ")\n",
        "\n",
        "logger = TBLogger(hparams.logdir)\n",
        "graph = tf.Graph()\n",
        "session = tf.Session(graph=graph)\n",
        "with graph.as_default():\n",
        "    train_step, metrics = model_fn(hparams)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(hparams.n_iterations):\n",
        "    batch_x, batch_y = mnist.train.next_batch(hparams.batch_size)\n",
        "    feeds = {'inputs:0': batch_x, 'targets:0': batch_y}\n",
        "    session.run(train_step, feeds)\n",
        "\n",
        "    if i % hparams.n_print == 0:\n",
        "        feeds = {'inputs:0': batch_x, 'targets:0': batch_y}\n",
        "        train_metrics = session.run(metrics, feeds)\n",
        "        for key in train_metrics:\n",
        "            print (str(key) + \":  \" + str(train_metrics[key]))\n",
        "        print ('-')\n",
        "            \n",
        "        "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/utils.py:89: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-66fff74d63d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0106af30ba7a>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mfull_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'norm_sum' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJYVze8538Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M8VTAM738Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAikWzH38N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}